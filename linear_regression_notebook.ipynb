{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "I'm planning this tutorial to be fun and intuitive. No jargon will be entertained / practiced without prior and proper introduction. This is going to be as **`local`** as [`Angamaly Diaries`](http://www.imdb.com/title/tt6167894/) in that aspect. But I think it would be up-and-coming if the reader has basic knowledge in TensorFlow, some math and basic orchestration of a Machine Learning experiment . Added, optimization techniques are explained only in an intuitive level since the detailed information could stretch the scope of this tutorial too much. \n",
    "\n",
    "## Shameless plug but helpful\n",
    "\n",
    "Check out my basic tutorial on TensorFlow [here](https://github.com/sleebapaul/hello_tensorflow/blob/master/hello_tensorflow.ipynb) if you're not familiar with it.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, we'll try to understand Regression, specifically Linear Regression. Later, we'll solve an interesting problem using Regression with TensorFlow. Ultimately that's the point of learning, ain't it? Solving problems. \n",
    "\n",
    "We'll start the tutorial with some high school math. \n",
    "\n",
    "Consider the function $$y = x$$. \n",
    "\n",
    "We can write $y$ as $f(x)$ since $y$ is defined on $x$, right? So, $$f(x) = x$$\n",
    "\n",
    "What would be the value $f(x)$ for a particular $x$ ? Consider the following table and have a keen look on the values. I've put 10 data points of the function $f(x)$ in this table. \n",
    "\n",
    "| x \t| f(x) \t|\n",
    "|----\t|------\t|\n",
    "| 1 \t| 1 \t|\n",
    "| 0 \t| 0 \t|\n",
    "| 3 \t| 3 \t|\n",
    "| 10 \t| 10 \t|\n",
    "| -1 \t| -1 \t|\n",
    "| -8 \t| 8 \t|\n",
    "| 6 \t| 6 \t|\n",
    "| -2 \t| -2 \t|\n",
    "| 9 \t| 9 \t|\n",
    "\n",
    "How this function would look like? Let's plot it right?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e553198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 0, 3, 10, -1, -8, 6, -2, 9]) # defined x as a numpy array of the table values\n",
    "y = x # y = f(x) = x\n",
    "\n",
    "plt.scatter(x, y) # let's plot the graph\n",
    "\n",
    "plt.xlabel('x') # xlabel\n",
    "plt.ylabel('f(x)')# y label\n",
    "plt.title('Line plot of f(x) = x') # title of plot\n",
    "plt.grid(True) # grid on plot surface\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\"Aha ! cool. Easy hit. Now let's do the reverse, shall we? \"\n",
    "\"What do you mean by reverse? We've a function. We plotted it. End of the story.\"\n",
    "\"No. This is the starting of our story :D . Primarily,  let me elaborate what I meant by reverse.\" . \n",
    "```\n",
    "\n",
    "Here we've the function $f(x) = x$, with that we calculated the data points i.e. $y$ values for each $x$. We even plotted it. Now, imagine what if we don't know what the function is. All we have is some data points. Can we calculate the function/relationship from those data points? That's what **reverse** means. \n",
    "\n",
    "```\n",
    "\"How can we do THAT ? :/\"\n",
    "\"Haha ! That is called Curve Fitting.\"\n",
    "\"Wait, we've started with Regression and now you're talking about Curve fitting. Are you okay, Sleeba?\"\n",
    "\"Yeah, I'm. Read on :D\"\n",
    "```\n",
    "\n",
    "## Curve Fitting \n",
    "\n",
    "[Curve fitting](https://en.wikipedia.org/wiki/Curve_fitting) is the process of constructing a curve, or a mathematical function, that has the **best fit** to a series of data points, possibly subject to constraints. \n",
    "\n",
    "Fancy definition right? Terms like best fit, subjects to constraints, blah blah blah ... :D Don't think about any of it right now. Let's see what's the exact problem we are addressing here. Consider the table again. \n",
    "\n",
    "| x \t| f(x) \t|\n",
    "|----\t|------\t|\n",
    "| 1 \t| 1 \t|\n",
    "| 0 \t| 0 \t|\n",
    "| 3 \t| 3 \t|\n",
    "| 10 \t| 10 \t|\n",
    "| -1 \t| -1 \t|\n",
    "| -8 \t| 8 \t|\n",
    "| 6 \t| 6 \t|\n",
    "| -2 \t| -2 \t|\n",
    "| 9 \t| 9 \t|\n",
    "\n",
    "\n",
    "All we know is these data points. Say, when $x$ = 1, $f(x)$ would be 1 or when $x$ = -1 then $f(x)$ would be -1. \n",
    "\n",
    "What we need to understand from these points is a relationship between $x$ and $f(x)$. Fortunately, we don't have any clue about that relationship. \n",
    "\n",
    "```\n",
    "\"No, we know that it's a straight line.\"   \n",
    "\"How do we know that?\"  \n",
    "\"Because we generated data points for f(x) = x and plotted it as a straight line just now :P :D\"  \n",
    "\"Not that easy, girl! What about this?? :D\"  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11932e278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.randint(5, size=10) # defined x as a numpy array of the table values\n",
    "y = np.random.randint(5, size=10) # \n",
    "\n",
    "plt.scatter(x, y) # let's plot the graph\n",
    "\n",
    "plt.xlabel('x') # xlabel\n",
    "plt.ylabel('f(x)')# y label\n",
    "plt.title('Line plot of y = Unknown(x)') # title of plot\n",
    "plt.grid(True) # grid on plot surface\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuts and Bolts of Curve Fitting\n",
    "\n",
    "We need to construct a relationship between $x$ and $y$ using the data points available for us. It could be a linear relationship, maybe an exponential growth or a highly unpredictable random distribution. So how do we start ? \n",
    "\n",
    "There is a systematic procedure for curve fitting. \n",
    "\n",
    "---------------------------\n",
    "### I.  Plot the data points\n",
    "    - This will help us to understand the basic relationship between the variables\n",
    "\n",
    "Eg. Consider the distribution below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/linear_scatter_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the relationship?\n",
    "1. Exponential\n",
    "2. Linear growth\n",
    "3. Linear decay\n",
    "4. Polynomial \n",
    "\n",
    "Clearly it is a linear relationship and it a growth.  \n",
    "What about this ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/rsz_exp_decay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearly decaying ? Or exponentially decaying? Well we'll settle on decaying :D   \n",
    "\n",
    "This is an issue. So we should accept the fact that we can't always understand the underlying relationships between variables just by plotting the scatter plot. These are 2-D plots or maybe even we can manage 3-D, but definitely not, say, a 10-D plot.\n",
    "\n",
    "-----------------------------\n",
    "### II. We'll assume a relationship for variables to start with.  \n",
    "    - The relationship could be Linear or Exponential or Polynomial or it could be anything.  \n",
    "    \n",
    "Now this is confusing. How that works? We don't know the relationship between the variables, how can we just \"ASSUME\" something? \n",
    "\n",
    "Before answering that question we need to know about two curve fitting paradigms. \n",
    "\n",
    "**Types of Curve Fitting**\n",
    "\n",
    "1. [Interpolation](https://en.wikipedia.org/wiki/Interpolation)\n",
    "2. Regression  \n",
    "\n",
    "```\n",
    "\"WOO-HOO! Regression came to picture. Now dots are connecting. \"\n",
    "\"I told you :D\"\n",
    "```\n",
    "\n",
    "Interpolation and Regression are differed by the problem they solve. See the picture below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "![title](images/inter_regress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Difference**\n",
    "\n",
    "As we see in the picture, both of the curves fit the same set of data points. But there is a difference in the way they are fitted. The left is Interpolation and other is Regression. Now think about the difference. \n",
    "\n",
    "In Interpolation, we make sure that each data point is perfectly fitted in the curve we're fitting. In constrast, Regession model is not strict about each point fitting the curve, rather we focus on the **trend** of the entire range of data.  \n",
    "\n",
    "You may ask if it is an approximation. Of course, Regression is approximation, but it is a benefitting approximation.  \n",
    "```\n",
    "\"I need more explanation :(\"\n",
    "\"Okay :)\"\n",
    "```\n",
    "\n",
    "See, if you look deep enough, you can interpret that, Interpolation is more concerned about relationships between immediate data points, not with the entire data. Concurrently, Regression showcase main tendencies in the complete data set. \n",
    "\n",
    "How this subtle difference matters in a real life problem? \n",
    "\n",
    "A good example for why you might want to approximate instead of interpolate are prices on stock market. \n",
    "\n",
    "Say, you can take stoke prices of Apple from 1991 to 2017, and try to interpolate them to get some prediction of the price in 2018. This is rather a bad idea, because there is no reason to think that the relations between the prices can be exactly expressed by a polynomial. But linear regression might do the trick as the linear function might be a good approximation.  \n",
    "\n",
    "Between, the curve we fit using Regression is also called Regression line (JARGON ALERT :P)\n",
    "\n",
    "Back to our initial question, how we can just ASSUME a relationship to an unknown set of data points given to us? \n",
    "\n",
    "Here, we should understand the intention of our assumption. We're not investigating the exact curve on which all the data points would fit into. Rather, we are trying to understand the behaviour of the data. Maybe, the data is exponential and we assumed a linear relationship. But with that assumption, we're actually trying to derive the best fitted straight line or in other words, the best linear approximation with which the data can be described. \n",
    "\n",
    "Readers who are familiar with Machine Learning can relate [overfitting](https://en.wikipedia.org/wiki/Overfitting) with interpolation. \n",
    "\n",
    "Now keep calm and listen to Morpheus :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/meme.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Finding the best fit\n",
    "\n",
    "From starting onwards we are hearing this term `best fit`. What do we mean by a best fit? Consider the literal meaning of `best fit` and make a guess on following pictures.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/best_fit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be best fit for the data?\n",
    "\n",
    "```\n",
    "A, B or C ?\n",
    "\"I think it is A. \n",
    "```\n",
    "Intuition of best fit is that, the best fitted curve would be most adequate assumed approximation for the input data points according to a metric.\n",
    "\n",
    "```\n",
    "Metric? Why do we need a metric? It's easy. We can get it from just seeing it.\"\n",
    "\"Really! then try this? ;)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/best_fit_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\"Now temme? A, B or C? :P\"\n",
    "\"Meh. You got me this time :/\"\n",
    "```\n",
    "So how do we calculate the best fit for distributions like above? Just seeing the fitted curve won't help on complex distributions. What would be that metric to measure adequacy of fitness? Don't worry, we've a systematic procedure to find the best fitting curve. \n",
    "\n",
    "1. Assume the relationship and initialize it's coefficients.\n",
    "2. Defining a cost function to measure the fitness.\n",
    "2. Find the optimum coefficients which minimize value of that cost iteratively. \n",
    "\n",
    "```\n",
    "Coefficients, Cost, Relationship ... What the hell ?? !!!\n",
    "Don't panic kiddo. We're on right path :D\n",
    "```\n",
    "\n",
    "From here onwards, we'll converge to Single Variable/Invariate Linear Regression to explain the procedures. The procedures are same for any kind of Regression, but variations will be on assumed functions, cost and optimization. You may find a variety of Regression models [here.](https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/)  \n",
    "\n",
    "---------------------------------\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "Let me serve you some jargon delicacy for starters :D  \n",
    "\n",
    "Linear Regression establishes a relationship between dependent variable ($y$) and one or more independent variables ($x$) using a best fit straight line.  \n",
    "\n",
    "Aha! Now our $x$ and $y$ have cool names. Why they've such names? Its quite self explanatory.\n",
    "\n",
    "Consider our old data points again.\n",
    "\n",
    "| x \t| y \t|\n",
    "|----\t|------\t|\n",
    "| 1 \t| 1 \t|\n",
    "| 0 \t| 0 \t|\n",
    "| 3 \t| 3 \t|\n",
    "| 10 \t| 10 \t|\n",
    "| -1 \t| -1 \t|\n",
    "| -8 \t| 8 \t|\n",
    "| 6 \t| 6 \t|\n",
    "| -2 \t| -2 \t|\n",
    "| 9 \t| 9 \t|\n",
    "\n",
    "$x$ can take any values for it as we see in the table. It is independent of any restrictions. But $y$ can take values depending upon its relatioship with $x$. So $y$ is dependent. Simple :)\n",
    "\n",
    "**Assumed function**\n",
    "\n",
    "In Linear Regression, as Morpheus tells us, the curve we fit is a straight line and it is described as,\n",
    "\n",
    "$$y = ax + b$$\n",
    "\n",
    "Our assumed function has a funny name called Hypothesis function. Hypothesis of linear regression is a straight line. Then, coefficients of our hypothesis function would be,\n",
    "\n",
    "- $a$ = Slope of the line\n",
    "- $b$ = Y Intercept of the line\n",
    "\n",
    "The goal is to find the optimum values of $a$ and $b$, so that it will give us the best fit for the data. To find this optimum values, we need a metric to measure the quality of fit. And that metric is called a [cost function.](https://en.wikipedia.org/wiki/Loss_function). \n",
    "\n",
    "**Cost Function**\n",
    "\n",
    "We define a cost function to measure the fitness quality of the curve we fit each time. The best fit should be obviously at minimum cost. Now think about it logically. What would be a good cost function ?  \n",
    "\n",
    "I think it is the error between what we expect and what the assumption give us. Confused ? We'll start with error at each point. Consider the error proposed below. What do you think about it's credibility?\n",
    "\n",
    "$$Error\\ =\\ True\\ value\\ of\\ Y\\ -\\ Assumed\\ value\\ of\\ Y$$\n",
    "\n",
    "Let me explain this step by step with the above data.\n",
    "\n",
    "Say we initialized the curve as $$y = 2x + 1$$\n",
    "\n",
    "Now, for $x = 1$, from the table, $y\\_true = 1$. What would be predicted  value for y with the assumed function? $$y\\_pred = 2 * 1 + 1 = 3$$\n",
    "\n",
    "Then error at $x = 1$ would be \n",
    "$$error\\ =\\ y\\_true\\ -\\ y\\_assumed$$\n",
    "$$=\\ 1\\ -\\ 3\\ =\\ -2$$\n",
    "\n",
    "Similarly for each points in the data points, there will an error associated with it as shown in the figure below. If we can reduce this error at each points, that means the difference between $y\\_true$ and $y\\_pred$ is reduced. That results in a line which is most adequate to fit that data point. Simple, right? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/error_linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll scale it to the entire dataset. What would be average error considering say, $m$ data points? \n",
    "\n",
    "$$Average\\ Error,\\ E\\ =\\ \\frac{1}{m}\\sum_{i=1}^{m} y\\_true^{(i)}\\ -\\ y\\_pred^{(i)}$$\n",
    "$$\\ =\\ \\frac{1}{m}\\sum_{i=1}^{m} y\\_true^{(i)}\\ -(\\ ax^{(i)}\\ +\\ b)$$\n",
    "where $i$ represent each data points. \n",
    "\n",
    "Though, considering the explanations yet, this is intuitive and expected cost function, it has a practical problem. The negative and positive errors will cancel out while averaging the entire dataset, results in net error to zero. So we'll consider tweaking it to another form called [Mean Squared Error.](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "\n",
    "The cost function becomes, $$Cost\\ =\\ \\frac{1}{m}\\sum_{i=1}^{m} (y\\_true^{(i)}\\ -\\ y\\_pred^{(i)})^{2} $$\n",
    "\n",
    "Now, even the error is negative, it won't cancel out with positive errors as we square the difference. Now comes the procedure finding minimum value of this cost and coefficients $a_{opt}$ and $b_{opt}$ corresponding to that minimum cost. Then our best fit would be,\n",
    "$$y\\ =\\ a_{opt}\\ *\\ x\\ +\\ b_{opt}$$\n",
    "\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "You all have studied linear algebra in high school. We are going to apply a bit of it on the above cost function.\n",
    "\n",
    "```\n",
    "Why? What is wrong now?\n",
    "Well, it's all about arrangement of convenience baby ;)\n",
    "```\n",
    "\n",
    "What is the arrangement of convenience here? Let me explain. \n",
    "\n",
    "$$ 5\\ *\\ 3\\ =\\ 15$$   \n",
    "$$2\\ *\\ 4\\ =\\ 8$$  \n",
    "$$7\\ *\\ 1\\ =\\ 7$$\n",
    "\n",
    "Now I'm representing it as something you're familiar. \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "5\\\\ \n",
    "2\\\\ \n",
    "7\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "3\\\\ \n",
    "4\\\\ \n",
    "1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "15\\\\ \n",
    "8\\\\ \n",
    "7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Yes, element wise matrix multiplication. Let's try something more interesting ;) Consider the below table. \n",
    "\n",
    "\n",
    "| x \t| y \t|\n",
    "|----\t|------\t|\n",
    "| 5 \t| 3 \t|\n",
    "| 2 \t| 4 \t|\n",
    "| 7 \t| 1 \t|\n",
    "\n",
    "\n",
    "\n",
    "We want to compute $$\\sum_{i=1}^{3} x^{(i)}\\ *\\ y^{(i)}$$\n",
    "\n",
    "What would be the matrix replica? ;) \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "5 & 2 & 7\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "3\\\\\n",
    "4\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "30\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Exactly, our little matrix multiplication. :D Now, how we are going to represent our cost function as matrix? \n",
    "\n",
    "Say we have $m$ data points, then we will define parameters for entire dataset as follows. \n",
    "\n",
    "\n",
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "x^{(1)}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "x^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$ Y_{true} = Y = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$cost\\ =\\ \\frac{1}{m} \\sum_{1}^{m}[Y - (a\\ *\\ X+b)]$$\n",
    "\n",
    "```\n",
    "This equation is looking the similar as our previous cost equation.\n",
    "Yes, it does.\n",
    "Then what is the point? \n",
    "Good question :) \n",
    "```\n",
    "\n",
    "I'll explain one advantage of vectorization quickly and list the others below. \n",
    "\n",
    "Consider the cost function of Multi Variable Linear Regression. You don't need to worry about the fancy jargon. Multi Variable Linear regression has more than one independent variables unlike our little sweet Single Variable Linear Regression.   \n",
    "\n",
    "$$Cost\\ =\\ \\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n} (\\ y_{true}\\ -\\ y_{pred})^{2}$$\n",
    "\n",
    "Here we have an additional sum to iterate through $(j)$, the various features / independent variables. A sum requires a `for loop`. So we need two `for loops` here. But using vectorization we can reduce the inner iteration through features. Dig deeper [here.](https://www.youtube.com/watch?v=iln_R5iJ1ts)\n",
    "\n",
    "Other reasons for vectorization are, \n",
    "\n",
    "* Almost all numerical computation libraries has n dimensional arrays basic data structures, thus implementation is easy.\n",
    "* Matrix operations are faster than element wise computation.\n",
    "* They can be parallelize to some extent to exploit multiple cpu's.\n",
    "\n",
    "------------------------------\n",
    "\n",
    "## Optimization techniques\n",
    "\n",
    "But you still don't know how `minimization of cost` works, do you? ;)\n",
    "\n",
    "Optimization techniques is, itself worth a few tutorials :D So, I would like to give you an intuition about what is going to happen at this level of Linear Regression and move on. References for learning them deeply will be added at the end of this section. \n",
    "\n",
    "Okay. An optimization techique is procedure of selecting the best element (with regard to some criterion) from some set of available alternatives. It is like choosing the best onion from a pile of onions (Ask your mother. She knows :D). \n",
    "\n",
    "Mathematically speaking, an optimization algorithm finds the minimum or maximum of a function. In our case we need to minimize the cost function. Consider the following picture. A boy is blindfolded and he's standing at the top of a mountain. He needs to reach the lake at the valley. What would be his path of travel? The initial position of boy is our initial parameter values. As the optimization progress, the boy moves downhill towards the lake, the [GLOBAL MINIMA](https://en.wikipedia.org/wiki/Maxima_and_minima)  and hopefully reach the destination. The global minimum point gives us $a_{opt}$ and $b_{opt}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/grad_desc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boy can't reach the valley in a single step, unless he had a slip :D. Likewise, none of the optimization techniques can reach the GLOBAL MINIMA in a single leap. There is a term called learning rate, which decides how big the step, the algorithm takes downhill at once, towards valley. We need to iterate over the different values of parameters and will eventually settle down with the most desirable value among them. Well, optimization techniques are one way of doing this process of finding minimum or maximum of a function. There are analytical ways to compute the optimum coefficients, which don't need iterations. Then why we use the messy way? Read about it yourself :) \n",
    "\n",
    "**Skelton of any optimization technique**\n",
    "\n",
    "1. Stopping_criteria = epsilon\n",
    "    - Eg. $\\epsilon\\ =\\ 10^{-7}$\n",
    "2. Initialize coefficients\n",
    "    - Mostly, it'll be a random intialization\n",
    "    - In our case $a$ and $b$\n",
    "3. Calculate cost function with current coefficients\n",
    "    - In our case, $Cost1\\ =\\ \\frac{1}{m}\\sum_{i=1}^{m} (y\\_true^{(i)}\\ -\\ (\\ ax^{(i)}\\ +\\ b))^{2} $\n",
    "4. Update the coeffients by minimizing or maximizing the cost function\n",
    "    - We'll be using an algorithm called [Gradient Descent](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/) in our coding section.\n",
    "    - We've, say, updated variables $a\\_new$ and $b\\_new$ after minimizing the cost.\n",
    "5. Calculate new cost using new coefficients\n",
    "    - $Cost2\\ =\\ \\frac{1}{m}\\sum_{i=1}^{m} (y\\_true^{(i)}\\ -\\ (\\ a\\_new\\ *\\ x^{(i)}\\ +\\ b\\_new))^{2} $\n",
    "6. If (Cost2 - Cost1) < epsilon, stop the process; else go to step 3\n",
    "    - We can stop the iteration by setting a definite number of iterations too\n",
    "    \n",
    "After each iteration, the cost will be reduced and coefficients will be more refined. If not, let me tell you mate, you're in trouble. :D One entire iteration through the data is called an epoch. \n",
    "\n",
    "### Sorry, but...\n",
    "\n",
    "1. For a novice to optimization techniques, this whole setup would look sloppy. You people would not be able to connect the dots completely :D I understand that pain since I've gone through it once. But that's completely fine. You need an intuition about what is going to happen and that's what I've provided above. Jump into the coding session and understand the implementation of the optimization algorithm. Thanks to #TensorFlow, it is just one line of code. \n",
    "\n",
    "2. Optimization techniques to a machine learning algorithm is engine to a car. As things progress, I highly recommend you to find some time for understanding them in greater detail. Some resources are added below in favor of that.\n",
    "\n",
    "1. [Keep it simple! How to understand Gradient Descent algorithm](https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html)\n",
    "2. [Gradient Descent For Machine Learning](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)\n",
    "3. [Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "4. [Introduction to Gradient Descent Algorithm (along with variants) in Machine Learning](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/)\n",
    "-----------------------------\n",
    "## Prediction \n",
    "\n",
    "Once we have an optimum model, we can make predictions on new values of independent variables. \n",
    "\n",
    "Back to our stock example, you can take stoke prices of Apple from 1991 to 2017, and try to predict prediction of the price in 2018. To learn our $a\\_opt$ and $b\\_opt$, we can use the data points from 1991 and 2017. Once it is found, \n",
    "\n",
    "$$y\\_pred\\_2018\\ =\\ a\\_opt\\ *\\ x\\_2018\\ +\\ b\\_opt$$  \n",
    "\n",
    "You won't be getting an idea of this part until you see it on a real problem. We'll demonstrate it in our coding session and trust me this is the fruit of all these pain we've been going through :D\n",
    "\n",
    "Now, learning the $a\\_opt$ and $b\\_opt$ from examples is called \"training\" phase and predicting for a new example using learned parameters is called \"testing\". For every Machine Learning in the universe, we've these two phases. \n",
    "\n",
    "## Final thoughts\n",
    "\n",
    "So that's it. The theory part of Linear Regression is over. We'll code all these blah blah blahs using TensorFlow to solve an interesting real life problem. So see you at coding [notebook.]() \n",
    "\n",
    "Here, implicitly I've surfaced most of the terms and procedures you would find in an ML experiment. Thus you may find it useful, if you're a novice to ML. \n",
    "\n",
    "Happy Learning !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
